{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "sys.path.append('./dataset/')\n",
    "from create_google_fonts_dataset import parse_gf_metadata, save_rendered_glyphs\n",
    "from classification_dataset import CharClassificationDataset\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "sys.path.append('./dataset/')\n",
    "from create_google_fonts_dataset import parse_gf_metadata, save_rendered_glyphs\n",
    "from classification_dataset import CharClassificationDataset\n",
    "\n",
    "# render fonts\n",
    "ofl_path = './dataset/fonts/ofl/'\n",
    "fonts_data = parse_gf_metadata(ofl_path) # google fonts dataframe\n",
    "\n",
    "# removing blacklisted fonts from the dataframe\n",
    "blacklist_fonts = ['Kumar One', 'Rubik'] # fonts with broken tabels\n",
    "indeces_to_remove = False\n",
    "for font_name in blacklist_fonts:\n",
    "    indeces_to_remove += (fonts_data.name==font_name).values\n",
    "fonts_data.drop(np.where(indeces_to_remove)[0], inplace=True)\n",
    "\n",
    "# construct letter set\n",
    "capital_alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "char_set = capital_alphabet\n",
    "for char in 'OQMWIN': # removing problematic symbols\n",
    "    char_set = char_set.replace(char, '')\n",
    "\n",
    "# creating dataset\n",
    "tfs = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[np.mean([0.485, 0.456, 0.406])],\n",
    "                                 std=[np.mean([0.229, 0.224, 0.225])])                           \n",
    "])\n",
    "root_dir = './dataset/rendered_set/'\n",
    "dataset_c = CharClassificationDataset(fonts_data, root_dir, char_set, transform=tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "batch_size = 64\n",
    "\n",
    "data_size = len(dataset_c)\n",
    "validation_fraction = .2\n",
    "\n",
    "val_split = int(np.floor((validation_fraction) * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.seed(4)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_c, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=1, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_c, batch_size=batch_size,\n",
    "                                         sampler=val_sampler, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Classification_model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone.forward(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter features.0.weight requires grad\n",
      "parameter features.0.bias requires grad\n",
      "parameter classifier.6.weight requires grad\n",
      "parameter classifier.6.bias requires grad\n"
     ]
    }
   ],
   "source": [
    "# prepare backbone\n",
    "vgg11 = models.vgg11(pretrained=True).requires_grad_(False)\n",
    "vgg11.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "vgg11.classifier[-1] = torch.nn.Linear(in_features=4096, out_features=len(char_set), bias=True)\n",
    "for name, param in vgg11.named_parameters():\n",
    "    if not param.requires_grad: continue\n",
    "    print('parameter', name, 'requires grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "   | Name                  | Type              | Params\n",
      "--------------------------------------------------------\n",
      "0  | backbone              | VGG               | 128 M \n",
      "1  | backbone.features     | Sequential        | 9 M   \n",
      "2  | backbone.features.0   | Conv2d            | 640   \n",
      "3  | backbone.features.1   | ReLU              | 0     \n",
      "4  | backbone.features.2   | MaxPool2d         | 0     \n",
      "5  | backbone.features.3   | Conv2d            | 73 K  \n",
      "6  | backbone.features.4   | ReLU              | 0     \n",
      "7  | backbone.features.5   | MaxPool2d         | 0     \n",
      "8  | backbone.features.6   | Conv2d            | 295 K \n",
      "9  | backbone.features.7   | ReLU              | 0     \n",
      "10 | backbone.features.8   | Conv2d            | 590 K \n",
      "11 | backbone.features.9   | ReLU              | 0     \n",
      "12 | backbone.features.10  | MaxPool2d         | 0     \n",
      "13 | backbone.features.11  | Conv2d            | 1 M   \n",
      "14 | backbone.features.12  | ReLU              | 0     \n",
      "15 | backbone.features.13  | Conv2d            | 2 M   \n",
      "16 | backbone.features.14  | ReLU              | 0     \n",
      "17 | backbone.features.15  | MaxPool2d         | 0     \n",
      "18 | backbone.features.16  | Conv2d            | 2 M   \n",
      "19 | backbone.features.17  | ReLU              | 0     \n",
      "20 | backbone.features.18  | Conv2d            | 2 M   \n",
      "21 | backbone.features.19  | ReLU              | 0     \n",
      "22 | backbone.features.20  | MaxPool2d         | 0     \n",
      "23 | backbone.avgpool      | AdaptiveAvgPool2d | 0     \n",
      "24 | backbone.classifier   | Sequential        | 119 M \n",
      "25 | backbone.classifier.0 | Linear            | 102 M \n",
      "26 | backbone.classifier.1 | ReLU              | 0     \n",
      "27 | backbone.classifier.2 | Dropout           | 0     \n",
      "28 | backbone.classifier.3 | Linear            | 16 M  \n",
      "29 | backbone.classifier.4 | ReLU              | 0     \n",
      "30 | backbone.classifier.5 | Dropout           | 0     \n",
      "31 | backbone.classifier.6 | Linear            | 81 K  \n",
      "/Users/davidyuk/.venvs/conflicting_learning/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidyuk/.venvs/conflicting_learning/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844b376f8b014ed2a1014b59b7816b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = Classification_model(backbone=vgg11)\n",
    "trainer = Trainer()\n",
    "trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
